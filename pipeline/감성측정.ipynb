{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AInA3bfhNX-Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "input_data_file = \"df_master_TM_proj_lemma.csv\"\n",
        "market_json_file = \"market_lexicon.json\"\n",
        "dictionary_json_file = \"sentiment_lexicon.json\"\n",
        "output_file = \"final_tone_analysis_result.csv\""
      ],
      "metadata": {
        "id": "RWb0hLvpVSBc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "print(\">>> 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...\")\n",
        "df = pd.read_csv(\n",
        "    input_data_file,\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "df = df.dropna(subset=['cleaned_text_lemma', 'doc_id'])\n",
        "print(f\"   - Î∂ÑÏÑù ÎåÄÏÉÅ Î¨∏Ïû•(Ìñâ) Í∞úÏàò: {len(df)}Í∞ú\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy-BtwCkNiNv",
        "outputId": "55d72fea-ffbd-4de1-eed0-fde6de9ad7df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...\n",
            "   - Î∂ÑÏÑù ÎåÄÏÉÅ Î¨∏Ïû•(Ìñâ) Í∞úÏàò: 171788Í∞ú\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Î¨∏Ïû• ÌÜ§ Í≥ÑÏÇ∞ Ìï®Ïàò (ÏãúÏû•/ÏÇ¨Ï†Ñ Í≥µÏö©)\n",
        "\n",
        "def calculate_sentence_tone(df_input, vocab, hawkish_w, dovish_w):\n",
        "    # 1. Î≤°ÌÑ∞Ìôî (Î¨∏Ïû•Î≥Ñ Îã®Ïñ¥ ÎπàÎèÑ Í≥ÑÏÇ∞)\n",
        "    vectorizer = CountVectorizer(vocabulary=vocab, ngram_range=(1, 5))\n",
        "    dtm = vectorizer.transform(df_input['cleaned_text_lemma'])\n",
        "\n",
        "    # 2. Î¨∏Ïû•Î≥Ñ Ï†êÏàò Ìï©ÏÇ∞ (ÎπàÎèÑ * Í∞ÄÏ§ëÏπò)\n",
        "    sent_h_score = dtm.dot(hawkish_w)\n",
        "    sent_d_score = dtm.dot(dovish_w)\n",
        "    total_score = sent_h_score + sent_d_score\n",
        "\n",
        "    # 3. Î¨∏Ïû• ÌÜ§ Í≥ÑÏÇ∞\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        sent_tones = (sent_h_score - sent_d_score) / total_score\n",
        "    return np.nan_to_num(sent_tones, 0) # NaN -> 0"
      ],
      "metadata": {
        "id": "uj9DB4iwVgSz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. [ÏãúÏû• Ï†ëÍ∑ºÎ≤ï] Î¨∏Ïû• -> Î¨∏ÏÑú Ïñ¥Ï°∞ Í≥ÑÏÇ∞\n",
        "\n",
        "print(\"\\n>>> 3. ÏãúÏû• Ï†ëÍ∑ºÎ≤ï(Market Approach) Í≥ÑÏÇ∞ ÏãúÏûë...\")\n",
        "\n",
        "if os.path.exists(market_json_file):\n",
        "    # (1) JSON Î°úÎìú\n",
        "    with open(market_json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        market_lexicon = json.load(f)\n",
        "\n",
        "    # (2) Îã®Ïñ¥Ïû• Î∞è Í∞ÄÏ§ëÏπò Ï†ïÎ†¨ (Vectorizer ÏàúÏÑúÏóê ÎßûÏ∂îÍ∏∞ ÏúÑÌï¥ ÌïÑÏàò)\n",
        "    m_vocab_raw = list(market_lexicon.keys())\n",
        "\n",
        "    # ÏàúÏÑú Î≥¥Ïû•ÏùÑ ÏúÑÌï¥ ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ±\n",
        "    temp_vec = CountVectorizer(vocabulary=m_vocab_raw, ngram_range=(1, 5))\n",
        "    feature_names = temp_vec.get_feature_names_out()\n",
        "\n",
        "    m_h_weights = []\n",
        "    m_d_weights = []\n",
        "\n",
        "    for word in feature_names:\n",
        "        info = market_lexicon.get(word, {})\n",
        "        pol = info.get('polarity')\n",
        "        score = info.get('score', 0)\n",
        "\n",
        "        if pol == 'hawkish':\n",
        "            m_h_weights.append(score)\n",
        "            m_d_weights.append(0)\n",
        "        elif pol == 'dovish':\n",
        "            m_h_weights.append(0)\n",
        "            m_d_weights.append(score)\n",
        "        else:\n",
        "            m_h_weights.append(0)\n",
        "            m_d_weights.append(0)\n",
        "\n",
        "    # (3) Î¨∏Ïû• ÌÜ§ Í≥ÑÏÇ∞ Ìï®Ïàò Ìò∏Ï∂ú\n",
        "    # (feature_namesÍ∞Ä Ï†ïÎ†¨Îêú Îã®Ïñ¥Ïû•Ïù¥ÎØÄÎ°ú vocab ÏûêÎ¶¨Ïóê ÎÑ£ÏäµÎãàÎã§)\n",
        "    df['Sent_Tone_Market'] = calculate_sentence_tone(\n",
        "        df, feature_names, np.array(m_h_weights), np.array(m_d_weights)\n",
        "    )\n",
        "\n",
        "    # (4) Î¨∏Ïû• ÎùºÎ≤®ÎßÅ (1: Îß§Ìåå, -1: ÎπÑÎëòÍ∏∞, 0: Ï§ëÎ¶Ω)\n",
        "    df['Sent_Label_Market'] = np.sign(df['Sent_Tone_Market'])\n",
        "\n",
        "    print(\"   ‚úÖ ÏãúÏû• Ï†ëÍ∑ºÎ≤ï Î¨∏Ïû• Î∂ÑÏÑù ÏôÑÎ£å\")\n",
        "\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è ÏãúÏû• Ï†ëÍ∑ºÎ≤ï ÏÇ¨Ï†Ñ ÌååÏùºÏù¥ ÏóÜÏñ¥ 0ÏúºÎ°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§.\")\n",
        "    df['Sent_Tone_Market'] = 0\n",
        "    df['Sent_Label_Market'] = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoXnnUpFV1kq",
        "outputId": "bdfb0652-a6cd-4513-d259-78c00a9b1496"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> 3. ÏãúÏû• Ï†ëÍ∑ºÎ≤ï(Market Approach) Í≥ÑÏÇ∞ ÏãúÏûë...\n",
            "   ‚úÖ ÏãúÏû• Ï†ëÍ∑ºÎ≤ï Î¨∏Ïû• Î∂ÑÏÑù ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n>>> 4. ÏÇ¨Ï†Ñ Ï†ëÍ∑ºÎ≤ï(Dictionary Approach) Í≥ÑÏÇ∞ ÏãúÏûë...\")\n",
        "\n",
        "if os.path.exists(dictionary_json_file):\n",
        "    # (1) JSON Î°úÎìú (ÏãúÏû• Ï†ëÍ∑ºÎ≤ïÍ≥º ÎèôÏùºÌïòÍ≤å Î≥ÄÍ≤Ω)\n",
        "    with open(dictionary_json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        dict_map = json.load(f)\n",
        "\n",
        "    print(f\"   - ÏÇ¨Ï†Ñ Î°úÎìú ÏôÑÎ£å! Ìè¨Ìï®Îêú Îã®Ïñ¥ Ïàò: {len(dict_map)}Í∞ú\")\n",
        "\n",
        "    # (2) Í∞ÄÏ§ëÏπò Î∞∞Ïó¥ ÏÉùÏÑ±\n",
        "    # JSON Íµ¨Ï°∞Í∞Ä {\"word\": {\"polarity\": \"hawkish\", \"score\": 1.5}, ...} ÎùºÍ≥† Í∞ÄÏ†ï\n",
        "    d_vocab_raw = list(dict_map.keys())\n",
        "\n",
        "    # ÏàúÏÑú Î≥¥Ïû•ÏùÑ ÏúÑÌï¥ ÏûÑÏãú Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä ÏÉùÏÑ±\n",
        "    temp_vec_d = CountVectorizer(vocabulary=d_vocab_raw, ngram_range=(1, 5))\n",
        "    feat_d = temp_vec_d.get_feature_names_out()\n",
        "\n",
        "    d_h_weights = []\n",
        "    d_d_weights = []\n",
        "\n",
        "    for word in feat_d:\n",
        "        info = dict_map.get(word, {})\n",
        "\n",
        "        # ÌÇ§ Ïù¥Î¶ÑÏù¥ ÏãúÏû• Ï†ëÍ∑ºÎ≤ïÍ≥º Í∞ôÎã§Î©¥ 'polarity', Îã§Î•¥Îã§Î©¥ 'label' Îì±ÏúºÎ°ú ÏàòÏ†ï ÌïÑÏöî\n",
        "        p = info.get('polarity', '') # ÌòπÏùÄ info.get('label', '')\n",
        "        s = info.get('score', 1.0)   # Ï†êÏàòÍ∞Ä ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í 1.0\n",
        "\n",
        "        if 'hawkish' in p.lower():\n",
        "            d_h_weights.append(s)\n",
        "            d_d_weights.append(0)\n",
        "        elif 'dovish' in p.lower():\n",
        "            d_h_weights.append(0)\n",
        "            d_d_weights.append(s)\n",
        "        else:\n",
        "            d_h_weights.append(0)\n",
        "            d_d_weights.append(0)\n",
        "\n",
        "    # (3) Î¨∏Ïû• ÌÜ§ Í≥ÑÏÇ∞ (Ìï®ÏàòÎäî ÏúÑÏóêÏÑú Ï†ïÏùòÌïú Í≤É Ïû¨ÏÇ¨Ïö©)\n",
        "    df['Sent_Tone_Dict'] = calculate_sentence_tone(\n",
        "        df, feat_d, np.array(d_h_weights), np.array(d_d_weights)\n",
        "    )\n",
        "    df['Sent_Label_Dict'] = np.sign(df['Sent_Tone_Dict'])\n",
        "    print(\"   ‚úÖ ÏÇ¨Ï†Ñ Ï†ëÍ∑ºÎ≤ï Î¨∏Ïû• Î∂ÑÏÑù ÏôÑÎ£å\")\n",
        "\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è '{dictionary_json_file}' ÌååÏùºÏù¥ ÏóÜÏñ¥ ÏÇ¨Ï†Ñ Ï†ëÍ∑ºÎ≤ïÏùÄ 0ÏúºÎ°ú Ï≤òÎ¶¨Ìï©ÎãàÎã§.\")\n",
        "    df['Sent_Tone_Dict'] = 0\n",
        "    df['Sent_Label_Dict'] = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AyxuUqeV6Fs",
        "outputId": "061a07f0-047e-42cf-b7a4-495e6a3140ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> 4. ÏÇ¨Ï†Ñ Ï†ëÍ∑ºÎ≤ï(Dictionary Approach) Í≥ÑÏÇ∞ ÏãúÏûë...\n",
            "   - ÏÇ¨Ï†Ñ Î°úÎìú ÏôÑÎ£å! Ìè¨Ìï®Îêú Îã®Ïñ¥ Ïàò: 15000Í∞ú\n",
            "   ‚úÖ ÏÇ¨Ï†Ñ Ï†ëÍ∑ºÎ≤ï Î¨∏Ïû• Î∂ÑÏÑù ÏôÑÎ£å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Î¨∏ÏÑú Îã®ÏúÑ ÏßëÍ≥Ñ Î∞è Ï†ÄÏû•\n",
        "\n",
        "print(\"\\n>>> 5. Î¨∏ÏÑúÎ≥Ñ ÏµúÏ¢Ö ÌÜ§ ÏßëÍ≥Ñ Ï§ë...\")\n",
        "\n",
        "# ÏßëÍ≥Ñ Ìï®Ïàò: (Îß§Ìåå Î¨∏Ïû• Ïàò - ÎπÑÎëòÍ∏∞ Î¨∏Ïû• Ïàò) / (Îß§Ìåå Î¨∏Ïû• Ïàò + ÎπÑÎëòÍ∏∞ Î¨∏Ïû• Ïàò)\n",
        "def aggregate_tone(label_series):\n",
        "    h_count = (label_series > 0).sum()\n",
        "    d_count = (label_series < 0).sum()\n",
        "    total = h_count + d_count\n",
        "\n",
        "    if total == 0: return 0\n",
        "    return (h_count - d_count) / total\n",
        "\n",
        "# doc_id Í∏∞Ï§Ä Í∑∏Î£πÌôî\n",
        "final_df = df.groupby('doc_id').agg({\n",
        "    'Sent_Label_Market': aggregate_tone,\n",
        "    'Sent_Label_Dict': aggregate_tone\n",
        "}).rename(columns={\n",
        "    'Sent_Label_Market': 'Market_Tone',\n",
        "    'Sent_Label_Dict': 'Dictionary_Tone'\n",
        "}).reset_index()\n",
        "\n",
        "# Ï†ÄÏû•\n",
        "final_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\n=======================================================\")\n",
        "print(f\"üéâ Î∂ÑÏÑù ÏôÑÎ£å! Í≤∞Í≥º ÌååÏùº: {output_file}\")\n",
        "print(f\"=======================================================\")\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "id": "sQMIano2YNRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234bedb4-abe8-4c28-c65a-8a7d21921363"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> 5. Î¨∏ÏÑúÎ≥Ñ ÏµúÏ¢Ö ÌÜ§ ÏßëÍ≥Ñ Ï§ë...\n",
            "\n",
            "=======================================================\n",
            "üéâ Î∂ÑÏÑù ÏôÑÎ£å! Í≤∞Í≥º ÌååÏùº: final_tone_analysis_result.csv\n",
            "=======================================================\n",
            "          doc_id  Market_Tone  Dictionary_Tone\n",
            "0  FOMC_20130918     0.392405        -0.975904\n",
            "1  FOMC_20131030     0.466667        -0.914062\n",
            "2  FOMC_20131218     0.328358        -0.842294\n",
            "3  FOMC_20140129     0.596730        -0.907455\n",
            "4  FOMC_20140319     0.455224        -0.839416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aL7E9-xxdgYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}