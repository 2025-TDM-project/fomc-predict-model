# -*- coding: utf-8 -*-
"""í† í”½ëª¨ë¸ë§í•˜ê¸°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GIKvxUEYChrkAOZvremYkls2-E0-_Ixf

1ï¸âƒ£ Collocation phrase ì‚¬ì „ ë§Œë“¤ê¸°

ğŸ‘‰ 4-gram ì¤‘ì‹¬ + 3-gram ë³´ì¡°

ğŸ“Œ ì—­í• 
	â€¢	â€œì •ì±… ë¬¸êµ¬ ë©ì–´ë¦¬â€ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë§Œë“¤ ì¤€ë¹„
"""

import pandas as pd

# ì‚¬ìš©í•  collocation clean íŒŒì¼
fourgram = pd.read_csv('fourgram_col_clean.csv')
trigram  = pd.read_csv('trigram_col_clean.csv')

# ìƒìœ„ Nê°œë§Œ ì‚¬ìš© (ì‹¤ë¬´ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ë²”ìœ„)
FOURGRAM_TOP = 1200
TRIGRAM_TOP  = 600

four_phrases = fourgram.sort_values(
    ['freq', 'pmi'], ascending=[False, False]
).head(FOURGRAM_TOP)['ngram'].tolist()

three_phrases = trigram.sort_values(
    ['freq', 'pmi'], ascending=[False, False]
).head(TRIGRAM_TOP)['ngram'].tolist()

# ê¸¸ì´ ê¸´ ê²ƒë¶€í„° ì¹˜í™˜í•´ì•¼ ê¹¨ì§ ë°©ì§€
phrase_list = sorted(
    set(four_phrases + three_phrases),
    key=lambda x: -len(x.split())
)

print('total phrases :', len(phrase_list))

import pandas as pd

df = pd.read_csv('df_master_TM_proj_lemma.csv')

"""2ï¸âƒ£ Phraseë¥¼ í…ìŠ¤íŠ¸ì— ì ìš© (underscore ì¹˜í™˜)

ğŸ“Œ ì—­í• 
	â€¢	federal open market committee
â†’ federal_open_market_committee
	â€¢	í† í”½ëª¨ë¸ì´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ í•™ìŠµ ê°€ëŠ¥
"""

def apply_phrases(text, phrases):
    if pd.isna(text):
        return text

    out = text
    for p in phrases:
        token = p.replace(' ', '_')
        out = out.replace(p, token)
    return out

df['text_for_topic'] = df['cleaned_text_lemma'].apply(
    lambda x: apply_phrases(x, phrase_list)
)

df[['cleaned_text_lemma', 'text_for_topic']].head()

"""3ï¸âƒ£ Hawk / Dove seed words ì •ë¦¬

ğŸ‘‰ ì½œë¡œì¼€ì´ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨

ğŸ“Œ ì—­í• 
	â€¢	í† í”½ì˜ ë°©í–¥ì„±(hawk/dove)ì„ ìœ ë„
	â€¢	â€œí† í”½ëª¨ë¸ë¡œ ë¶„ë¥˜ë¥¼ ì‹œë„í–ˆë‹¤â€ëŠ” ì„¤ëª…ì˜ í•µì‹¬ ê·¼ê±°
"""

import pandas as pd

fourgram = pd.read_csv('fourgram_col_clean.csv')
trigram  = pd.read_csv('trigram_col_clean.csv')
bigram   = pd.read_csv('bigram_col_clean.csv')

for dfc in [fourgram, trigram, bigram]:
    dfc.columns = [c.lower().strip() for c in dfc.columns]

hawk_patterns = [
    'inflation', 'price', 'rate', 'tighten', 'hike',
    'policy', 'interest', 'fund', 'stability'
]

dove_patterns = [
    'growth', 'employment', 'labor', 'unemployment',
    'slowdown', 'recession', 'easing', 'cut',
    'accommodat', 'support'
]

def extract_seed_candidates(
    dfc,
    patterns,
    min_freq,
    min_pmi,
    top_n
):
    mask = dfc['ngram'].str.contains(
        '|'.join(patterns),
        case=False,
        regex=True
    )

    candidates = (
        dfc[mask]
        .query('freq >= @min_freq and pmi >= @min_pmi')
        .sort_values(['freq', 'pmi'], ascending=[False, False])
        .head(top_n)
        .copy()
    )

    return candidates

hawk_4g = extract_seed_candidates(
    fourgram,
    hawk_patterns,
    min_freq=20,
    min_pmi=20,
    top_n=25
)

dove_4g = extract_seed_candidates(
    fourgram,
    dove_patterns,
    min_freq=20,
    min_pmi=20,
    top_n=25
)

hawk_3g = extract_seed_candidates(
    trigram,
    hawk_patterns,
    min_freq=30,
    min_pmi=10,
    top_n=20
)

dove_3g = extract_seed_candidates(
    trigram,
    dove_patterns,
    min_freq=30,
    min_pmi=10,
    top_n=20
)

def to_seed_tokens(df_list):
    seeds = set()
    for dfc in df_list:
        for ng in dfc['ngram']:
            seeds.add(ng.replace(' ', '_'))
    return sorted(seeds)

hawk_seeds_auto = to_seed_tokens([hawk_4g, hawk_3g])
dove_seeds_auto = to_seed_tokens([dove_4g, dove_3g])

print('hawk seeds :', len(hawk_seeds_auto))
print('dove seeds :', len(dove_seeds_auto))

pd.DataFrame({
    'hawk_seeds': hawk_seeds_auto[:30],
    'dove_seeds': dove_seeds_auto[:30]
})

seed_topics = {
    "hawkish": hawk_seeds_auto,
    "dovish": dove_seeds_auto
}

# ì´ë¯¸ ìƒì„±ëœ ìë™ seed
hawk_auto = hawk_seeds_auto
dove_auto = dove_seeds_auto

print("auto hawk seeds:", len(hawk_auto))
print("auto dove seeds:", len(dove_auto))

hawk_manual = [
    "rate_hike",
    "tightening",
    "inflation",
    "price_stability",
    "federal_reserve",
    "fomc"
]

dove_manual = [
    "rate_cut",
    "easing",
    "accommodation",
    "labor_market",
    "unemployment_rate",
    "recession"
]

hawk_seeds_final = sorted(set(hawk_auto + hawk_manual))
dove_seeds_final = sorted(set(dove_auto + dove_manual))

print("final hawk seeds:", len(hawk_seeds_final))
print("final dove seeds:", len(dove_seeds_final))

seed_topics = {
    "hawkish": hawk_seeds_final,
    "dovish": dove_seeds_final
}

pip install bertopic

texts = (
    df['text_for_topic']
    .fillna('')
    .astype(str)
    .tolist()
)

# ì•ˆì „ ì²´í¬
print(type(texts), type(texts[0]))

# texts ë§Œë“¤ê¸° ì „ì— df['text_for_topic']ê°€ ì–´ë–¤ íƒ€ì…ì„ ê°–ëŠ”ì§€ ì ê²€
raw = df['text_for_topic'].tolist()

bad_idx = []
for i, x in enumerate(raw[:2000]):  # ë„ˆë¬´ í¬ë©´ ì•ë¶€ë¶„ë§Œ ê²€ì‚¬
    if not isinstance(x, str):
        bad_idx.append((i, type(x), x))

print('non-str samples:', len(bad_idx))
print(bad_idx[:5])

import pandas as pd

def force_text(x):
    # NaN/None
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""

    # í† í° ë¦¬ìŠ¤íŠ¸/íŠœí”Œì´ë©´ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹˜ê¸° (SentenceTransformerê°€ pairë¡œ ì˜¤í•´í•˜ëŠ” ê²ƒ ë°©ì§€)
    if isinstance(x, (list, tuple)):
        return " ".join([str(t) for t in x if t is not None])

    # ê·¸ ì™¸ëŠ” ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜
    return str(x)

# text_for_topicì„ ì•ˆì „ ë¬¸ìì—´ë¡œ ê³ ì •
df['text_for_topic'] = df['text_for_topic'].apply(force_text)

# BERTopic ì…ë ¥ìš© List[str]
texts = df['text_for_topic'].tolist()

# ìµœì¢… ì•ˆì „ ì²´í¬ (ë°˜ë“œì‹œ 0ì´ì–´ì•¼ ì •ìƒ)
print('non-str count:', sum([0 if isinstance(t, str) else 1 for t in texts]))
print('sample:', texts[0][:120])

pair_like = []
for i, t in enumerate(texts[:2000]):
    if isinstance(t, (list, tuple)):
        pair_like.append((i, t))
print('pair-like:', len(pair_like), pair_like[:3])

# =========================
# 1) ì…ë ¥ í…ìŠ¤íŠ¸ ê°•ì œ ì •ì œ (pandas ì¤‘ì‹¬)
# =========================
s = df['text_for_topic']

# None/NaN -> ""
s = s.fillna('')

# list/tuple -> ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸° (sentence pair ì˜¤í•´ ë°©ì§€)
s = s.map(lambda x: " ".join([str(t) for t in x if t is not None]) if isinstance(x, (list, tuple)) else x)

# ë‚˜ë¨¸ì§€ íƒ€ì… -> str ê°•ì œ
s = s.astype(str)

# =========================
# 2) ë¹ˆ ë¬¸ì„œ ì œê±° (ë©ˆì¶”ì§€ ì•Šê²Œ)
# =========================
mask = s.str.strip().ne('')
df_keep = df.loc[mask].copy()
s_keep = s.loc[mask]

print('all docs :', len(df))
print('kept docs:', len(df_keep))
print('dropped :', len(df) - len(df_keep))

# =========================
# 3) ê¸¸ì´ ì œí•œ (í† í° ê¸°ì¤€: ì •í™•í•˜ì§€ë§Œ split ë¹„ìš© ìˆìŒ)
#    - ë¹„ìš© ì¤„ì´ë ¤ë©´ "ìƒ˜í”Œ fit -> ì „ì²´ transform" êµ¬ì¡°ë¡œ ê°
# =========================
def cap_length(text, max_tokens=256):
    toks = text.split()
    return " ".join(toks[:max_tokens])

# ìµœì¢… ì…ë ¥ list[str]
texts_keep = s_keep.map(lambda x: cap_length(x, max_tokens=256)).tolist()

import pandas as pd
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer

# =========================
# 4) Vectorizer + BERTopic
# =========================
vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    min_df=10,
    stop_words="english"
)

# Define the embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

topic_model = BERTopic(
    nr_topics=15,
    vectorizer_model=vectorizer,
    embedding_model=embedding_model, # Now 'embedding_model' is defined
    seed_topic_list=list(seed_topics.values()),
    calculate_probabilities=True,
    verbose=True
)

# =========================
# 5) 2ë‹¨ê³„: ìƒ˜í”Œ fit -> ì „ì²´ transform
# =========================
sample_n = 30000
if len(texts_keep) < sample_n:
    sample_n = len(texts_keep)

topics_s, probs_s = topic_model.fit_transform(texts_keep[:sample_n])
print('sample fit :', len(topics_s), probs_s.shape)

topics_all, probs_all = topic_model.transform(texts_keep)
print('full transform :', len(topics_all), probs_all.shape)

df_keep['topic_id'] = topics_all
df_keep.to_csv('df_topic_result.csv', index=False)
print('saved : df_topic_result.csv')

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer

embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device="cuda")

vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    min_df=10,
    stop_words="english"
)

topic_model = BERTopic(
    nr_topics=15,
    vectorizer_model=vectorizer,
    embedding_model=embedding_model,
    seed_topic_list=list(seed_topics.values()),
    calculate_probabilities=False,   # âœ… ë©ˆì¶”ëŠ” êµ¬ê°„ ì œê±°
    verbose=True
)

sample_n = 30000
if len(texts_keep) < sample_n:
    sample_n = len(texts_keep)

topic_model.fit(texts_keep[:sample_n])
topics_all, _ = topic_model.transform(texts_keep)

df_keep['topic_id'] = topics_all
df_keep.to_csv('df_topic_result_topicid.csv', index=False)
print('saved : df_topic_result_topicid.csv')

"""ì‹œë“œ í¬ë§· í†µì¼ ë° ì¬ì •ë¦¬"""

import pandas as pd

# -------------------------
# 1) collocation clean ë¡œë“œ
# -------------------------
fourgram = pd.read_csv('fourgram_col_clean.csv')
trigram  = pd.read_csv('trigram_col_clean.csv')
bigram   = pd.read_csv('bigram_col_clean.csv')

for dfc in [fourgram, trigram, bigram]:
    dfc.columns = [c.lower().strip() for c in dfc.columns]

# -------------------------
# 2) phrase list ë§Œë“¤ê¸° (3g + 4g)
# -------------------------
FOURGRAM_TOP = 1200
TRIGRAM_TOP  = 600

four_phrases = fourgram.sort_values(['freq','pmi'], ascending=[False, False]).head(FOURGRAM_TOP)['ngram'].tolist()
three_phrases = trigram.sort_values(['freq','pmi'], ascending=[False, False]).head(TRIGRAM_TOP)['ngram'].tolist()

phrase_list = sorted(set(four_phrases + three_phrases), key=lambda x: -len(x.split()))
print('total phrases :', len(phrase_list))

def apply_phrases(text, phrases):
    if pd.isna(text) or text is None:
        return ""
    out = str(text)
    for p in phrases:
        out = out.replace(p, p.replace(' ', '_'))
    return out

df['text_for_topic'] = df['cleaned_text_lemma'].apply(lambda x: apply_phrases(x, phrase_list))

# -------------------------
# 3) auto seed ì¶”ì¶œ
# -------------------------
hawk_patterns = ['inflation','price','rate','tighten','hike','policy','interest','fund','stability']
dove_patterns = ['growth','employment','labor','unemployment','slowdown','recession','easing','cut','accommodat','support']

def extract_seed_candidates(dfc, patterns, min_freq, min_pmi, top_n):
    mask = dfc['ngram'].str.contains('|'.join(patterns), case=False, regex=True)
    out = (dfc[mask]
           .query('freq >= @min_freq and pmi >= @min_pmi')
           .sort_values(['freq','pmi'], ascending=[False, False])
           .head(top_n)
           .copy())
    return out

hawk_4g = extract_seed_candidates(fourgram, hawk_patterns, min_freq=20, min_pmi=20, top_n=25)
dove_4g = extract_seed_candidates(fourgram, dove_patterns, min_freq=20, min_pmi=20, top_n=25)
hawk_3g = extract_seed_candidates(trigram,  hawk_patterns, min_freq=30, min_pmi=10, top_n=20)
dove_3g = extract_seed_candidates(trigram,  dove_patterns, min_freq=30, min_pmi=10, top_n=20)

def to_seed_tokens(df_list):
    seeds = set()
    for d in df_list:
        for ng in d['ngram'].tolist():
            seeds.add(ng.replace(' ', '_'))   # âœ… text_for_topicê³¼ í¬ë§· í†µì¼
    return sorted(seeds)

hawk_auto = to_seed_tokens([hawk_4g, hawk_3g])
dove_auto = to_seed_tokens([dove_4g, dove_3g])

print('auto hawk seeds:', len(hawk_auto))
print('auto dove seeds:', len(dove_auto))

# -------------------------
# 4) manual seed í•©ì¹˜ê¸°
# -------------------------
hawk_manual = ["rate_hike","tightening","inflation","price_stability","federal_reserve","fomc"]
dove_manual = ["rate_cut","easing","accommodation","labor_market","unemployment_rate","recession"]

hawk_seeds_final = sorted(set(hawk_auto + hawk_manual))
dove_seeds_final = sorted(set(dove_auto + dove_manual))

print('final hawk seeds:', len(hawk_seeds_final))
print('final dove seeds:', len(dove_seeds_final))

seed_topics = {
    "hawkish": hawk_seeds_final,
    "dovish": dove_seeds_final
}

text_blob = " ".join(df['text_for_topic'].head(5000).tolist())
print('hawk seed hit:', sum([1 for s in seed_topics['hawkish'] if s in text_blob]))
print('dove seed hit:', sum([1 for s in seed_topics['dovish'] if s in text_blob]))

# =========================
# 1) í† í”½ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ
# =========================
topic_words = {}

for tid in set(df_keep['topic_id']):
    if tid == -1:
        continue
    words = [w for w, _ in topic_model.get_topic(tid)]
    topic_words[tid] = words


# =========================
# 2) í† í”½ë³„ hawk / dove ì ìˆ˜ ê³„ì‚°
# =========================
def topic_hawk_dove_score(words, hawk_seeds, dove_seeds):
    h = sum(1 for w in words if w in hawk_seeds)
    d = sum(1 for w in words if w in dove_seeds)
    return (h - d) / (h + d + 1)


topic_score = {
    tid: topic_hawk_dove_score(words, hawk_seeds_final, dove_seeds_final)
    for tid, words in topic_words.items()
}

# í™•ì¸
pd.DataFrame({
    'topic_id': list(topic_score.keys()),
    'hawk_dove_score': list(topic_score.values())
}).sort_values('hawk_dove_score', ascending=False)

def explode_seeds(seeds):
    out = set()
    for s in seeds:
        out.update(s.split('_'))
    return out

hawk_seed_words = explode_seeds(hawk_seeds_final)
dove_seed_words = explode_seeds(dove_seeds_final)

def topic_hawk_dove_score(words, hawk_words, dove_words):
    h = sum(1 for w in words if w in hawk_words)
    d = sum(1 for w in words if w in dove_words)
    return (h - d) / (h + d + 1)

topic_score = {
    tid: topic_hawk_dove_score(words, hawk_seed_words, dove_seed_words)
    for tid, words in topic_words.items()
}

# í† í”½ í•˜ë‚˜ ì§ì ‘ í™•ì¸
tid = list(topic_words.keys())[0]
print("topic words:", topic_words[tid][:15])

print("hawk seed words sample:", list(hawk_seed_words)[:15])
print("dove seed words sample:", list(dove_seed_words)[:15])

pd.Series(topic_score).describe()

pd.DataFrame({
    'topic_id': topic_score.keys(),
    'hawk_dove_score': topic_score.values()
}).sort_values('hawk_dove_score')

import numpy as np
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

# =========================================================
# 0) ì…ë ¥ ê°€ì •
#   - df: ì›ë³¸ DF
#   - df['text_for_topic'] ë˜ëŠ” df['cleaned_text_lemma'] ì¤‘ í•˜ë‚˜ ì¡´ì¬
#   - seed_topics = {"hawkish": [...], "dovish": [...]}  (underscore í¬í•¨ ê°€ëŠ¥)
# =========================================================

# =========================
# 1) í…ìŠ¤íŠ¸ ì •ì œ(ë²¡í„°í™”)
# =========================
text_col = 'text_for_topic'
if text_col not in df.columns:
    text_col = 'cleaned_text_lemma'

s = df[text_col]

# NaN -> "" í•œë²ˆì— ì²˜ë¦¬ + ë¬¸ìì—´í™”
s = s.fillna("").astype(str)

# í† í°ìˆ˜ ìë¥´ê¸°(512) - apply ëŒ€ì‹  ë¹ ë¥´ê²Œ
# (split ìì²´ëŠ” í•„ìš”í•˜ì§€ë§Œ, ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ applyë³´ë‹¤ ë¹ ë¥¸ í¸)
texts = s.tolist()
texts = [" ".join(t.split()[:512]) for t in texts]

# ë¹ˆ ë¬¸ì„œ ì œê±°(ë©ˆì¶”ì§€ ì•Šê²Œ)
mask_keep = np.fromiter((len(t.strip()) > 0 for t in texts), dtype=bool, count=len(texts))
texts_keep = [t for t, ok in zip(texts, mask_keep) if ok]
df_keep = df.loc[mask_keep].copy()

print('all docs :', len(texts))
print('kept docs:', len(texts_keep))
print('dropped :', len(texts) - len(texts_keep))

# =========================================================
# 2) BERTopic (í™•ë¥  ON)
#    - calculate_probabilities=True  -> probs ìƒì„±
# =========================================================
embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device="cuda")

vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    min_df=10,
    stop_words="english"
)

topic_model = BERTopic(
    nr_topics=15,
    vectorizer_model=vectorizer,
    embedding_model=embedding_model,
    seed_topic_list=list(seed_topics.values()),
    calculate_probabilities=True,   # âœ… í™•ë¥  ON
    verbose=True
)

# ìƒ˜í”Œ í•™ìŠµ í›„ ì „ì²´ transform (ì‹œê°„ ì ˆì•½)
sample_n = 30000
if len(texts_keep) < sample_n:
    sample_n = len(texts_keep)

topic_model.fit(texts_keep[:sample_n])

topics_all, probs_all = topic_model.transform(texts_keep)
df_keep['topic_id'] = topics_all

print('topics_all:', len(topics_all))
print('probs_all shape:', probs_all.shape)

# =========================================================
# 3) í† í”½ ì ìˆ˜ ê³„ì‚°(ë¹ ë¥´ê²Œ)
#    - seed ë‹¨ì–´ set ë§Œë“¤ì–´ O(1) lookup
#    - í† í”½ ë‹¨ì–´ list í•œ ë²ˆë§Œ ê°€ì ¸ì˜¤ê¸°
# =========================================================
def explode_seed_words(seed_list):
    # seedê°€ "rate_hike"ë©´ rate, hike ë‘˜ ë‹¤ ë‹¨ì–´ë¡œ ì¸ì •
    out = set()
    for s in seed_list:
        for w in str(s).split('_'):
            if w:
                out.add(w)
    return out

hawk_words = explode_seed_words(seed_topics['hawkish'])
dove_words = explode_seed_words(seed_topics['dovish'])

# topic_infoë¡œ "í™•ë¥  í–‰ë ¬ì˜ ì»¬ëŸ¼ í† í”½ ìˆœì„œ" í™•ë³´ (ê°€ì¥ ì¤‘ìš”)
topic_info = topic_model.get_topic_info()
valid_topics = topic_info.loc[topic_info['Topic'] != -1, 'Topic'].tolist()

# score_vecì„ valid_topics ìˆœì„œëŒ€ë¡œ ì±„ìš°ê¸°
score_vec = np.zeros(len(valid_topics), dtype=np.float32)

# í† í”½ ë‹¨ì–´ëŠ” í•„ìš”í•œ í† í”½ë§Œ ë”± í•œë²ˆì”©
# get_topic(tid) -> [(word, weight), ...]
for i, tid in enumerate(valid_topics):
    ws = topic_model.get_topic(tid)
    if not ws:
        score_vec[i] = 0.0
        continue

    # top wordsë§Œ ì“°ë©´ ë” ì•ˆì •ì (ë„ˆë¬´ ê¸¸ê²Œ ë³´ë©´ ì¡ìŒ ì¦ê°€)
    words = [w for w, _ in ws[:30]]

    # set lookupìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¹´ìš´íŠ¸
    h = sum((w in hawk_words) for w in words)
    d = sum((w in dove_words) for w in words)

    score_vec[i] = (h - d) / (h + d + 1.0)

# =========================================================
# 4) ë¬¸ì„œ ì ìˆ˜ = probs @ score_vec  (ë²¡í„° ì—°ì‚°)
# =========================================================
# probs_all shape: (N_docs, N_topics)
# score_vec  shape: (N_topics,)
doc_score = probs_all.dot(score_vec)

# ì•ˆì •ì ìœ¼ë¡œ -1~1
doc_score = np.clip(doc_score, -1.0, 1.0)

df_keep['doc_hawk_dove_score_prob'] = doc_score

print('\nscore describe')
print(df_keep['doc_hawk_dove_score_prob'].describe())

# =========================================================
# 5) ì €ì¥
# =========================================================
df_keep.to_csv('df_topic_scored_prob_fast.csv', index=False)
print('\nsaved : df_topic_scored_prob_fast.csv')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# =========================
# 0) df_keep ë¡œë”©/í™•ì¸
# =========================
# ì´ë¯¸ df_keepê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì“°ê³ , ì—†ìœ¼ë©´ ì €ì¥íŒŒì¼ì—ì„œ ë¡œë“œ
try:
    df_keep
except NameError:
    df_keep = pd.read_csv('df_topic_scored_prob_fast.csv')

score_col = 'doc_hawk_dove_score_prob'
text_col = 'text_for_topic' if 'text_for_topic' in df_keep.columns else (
    'cleaned_text_lemma' if 'cleaned_text_lemma' in df_keep.columns else 'cleaned_text'
)

need_cols = [score_col, 'topic_id', text_col]
for c in need_cols:
    if c not in df_keep.columns:
        raise ValueError(c + ' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. df_keep.columns í™•ì¸ í•„ìš”')

# =========================
# 1) ì ìˆ˜ ë¶„í¬(íˆìŠ¤í† ê·¸ë¨)
# =========================
vals = df_keep[score_col].astype(float).fillna(0.0).values

plt.figure(figsize=(10, 4))
plt.hist(vals, bins=60)
plt.title('Document Hawk-Dove Score Distribution')
plt.xlabel('score (-1 ~ 1)')
plt.ylabel('count')
plt.show()

print('score describe')
print(df_keep[score_col].describe())

# =========================
# 2) ìƒ/í•˜ìœ„ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸
# =========================
TOPK = 20

df_sorted = df_keep.sort_values(score_col, ascending=False)

top_docs = df_sorted.head(TOPK)[[score_col, 'topic_id', text_col]].copy()
bot_docs = df_sorted.tail(TOPK)[[score_col, 'topic_id', text_col]].copy()

print('\n===== TOP docs (hawkish side) =====')
print(top_docs.to_string(index=False))

print('\n===== BOTTOM docs (dovish side) =====')
print(bot_docs.to_string(index=False))

import numpy as np
import pandas as pd

# =========================
# 0) topic_model ì¡´ì¬ ì²´í¬
# =========================
try:
    topic_model
except NameError:
    raise ValueError("topic_modelì´ í˜„ì¬ ì„¸ì…˜ì— ì—†ìŠµë‹ˆë‹¤. (í† í”½ ë‹¨ì–´ë¥¼ ë½‘ìœ¼ë ¤ë©´ topic_model ê°ì²´ê°€ í•„ìš”)")

# =========================
# 1) topic_idë³„ ìš”ì•½(ë¬¸ì„œ ìˆ˜/í‰ê·  ì ìˆ˜)
# =========================
score_col = 'doc_hawk_dove_score_prob'

topic_stats = (
    df_keep.groupby('topic_id')[score_col]
    .agg(['count', 'mean', 'median'])
    .reset_index()
    .rename(columns={'count': 'doc_count', 'mean': 'score_mean', 'median': 'score_median'})
)

# =========================
# 2) topic_idë³„ top words ì¶”ì¶œ
# =========================
def get_top_words(tid, topn=15):
    if tid == -1:
        return ""
    ws = topic_model.get_topic(int(tid))
    if not ws:
        return ""
    return ", ".join([w for w, _ in ws[:topn]])

topic_stats['top_words'] = topic_stats['topic_id'].apply(lambda x: get_top_words(x, topn=15))

# =========================
# 3) ë¼ë²¨ë§ ë£° (ì„ê³„ê°’ì€ ë„ˆê°€ ì¡°ì •)
#   - meanì´ +0.15 ì´ìƒ: hawkish
#   - meanì´ -0.15 ì´í•˜: dovish
#   - ê·¸ ì™¸: neutral
# =========================
TH = 0.15
def label_topic(x):
    if x >= TH:
        return 'hawkish'
    if x <= -TH:
        return 'dovish'
    return 'neutral'

topic_stats['topic_label'] = topic_stats['score_mean'].apply(label_topic)

# ë³´ê¸° ì¢‹ê²Œ ì •ë ¬
topic_stats = topic_stats.sort_values(['topic_label', 'score_mean'], ascending=[True, False])

print('\n===== Topic label table =====')
print(topic_stats[['topic_id', 'topic_label', 'doc_count', 'score_mean', 'score_median', 'top_words']].to_string(index=False))

# ì €ì¥
topic_stats.to_csv('topic_label_table.csv', index=False)
print('\nsaved : topic_label_table.csv')

TH = 0.05  # í•„ìš”í•˜ë©´ 0.03 ~ 0.08 ì‚¬ì´ì—ì„œ ì¡°ì •

def classify_doc(score, th=TH):
    if score >= th:
        return "hawkish"
    elif score <= -th:
        return "dovish"
    else:
        return "neutral"

df_keep['doc_policy_label'] = df_keep['doc_hawk_dove_score_prob'].apply(classify_doc)

df_keep['doc_policy_label'].value_counts(normalize=True).round(4)

required_cols = ['text_for_topic', 'topic_id', 'doc_hawk_dove_score_prob']
for c in required_cols:
    if c not in df_keep.columns:
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {c}")

df_keep['doc_hawk_dove_score_prob'].describe()

TH = 0.05  # ì„ê³„ê°’ (ë„ˆ ë°ì´í„° ê¸°ì¤€ ìµœì )

def classify_doc(score, th=TH):
    if score >= th:
        return "hawkish"
    elif score <= -th:
        return "dovish"
    else:
        return "neutral"

df_keep['doc_policy_label'] = df_keep['doc_hawk_dove_score_prob'].apply(classify_doc)

print("===== Document label distribution =====")
print(
    df_keep['doc_policy_label']
    .value_counts(normalize=True)
    .round(4)
)

topic_summary = (
    df_keep.groupby('topic_id')['doc_hawk_dove_score_prob']
    .agg(['count', 'mean', 'median'])
    .reset_index()
    .rename(columns={
        'count': 'doc_count',
        'mean': 'score_mean',
        'median': 'score_median'
    })
)

def label_topic(x):
    if x >= TH:
        return 'hawkish'
    elif x <= -TH:
        return 'dovish'
    else:
        return 'neutral'

topic_summary['topic_label'] = topic_summary['score_mean'].apply(label_topic)

topic_summary = topic_summary.sort_values(
    ['topic_label', 'score_mean'],
    ascending=[True, False]
)

print("\n===== Topic label table =====")
print(topic_summary.to_string(index=False))

df_keep.to_csv('df_doc_policy_label.csv', index=False)
topic_summary.to_csv('topic_label_table.csv', index=False)

print("saved : df_doc_policy_label.csv")
print("saved : topic_label_table.csv")

import matplotlib.pyplot as plt

score_col = 'doc_hawk_dove_score_prob'

plt.figure(figsize=(10, 5))
plt.hist(df_keep[score_col], bins=100, alpha=0.7)
plt.axvline(0.05, color='red', linestyle='--', label='hawk threshold (+0.05)')
plt.axvline(-0.05, color='blue', linestyle='--', label='dove threshold (-0.05)')
plt.axvline(0, color='black', linestyle=':', label='neutral (0)')
plt.title('Distribution of Hawk-Dove Document Scores')
plt.xlabel('Score')
plt.ylabel('Document Count')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.hist(
    df_keep[score_col],
    bins=100,
    range=(-0.2, 0.2),
    alpha=0.8
)
plt.axvline(0.05, color='red', linestyle='--')
plt.axvline(-0.05, color='blue', linestyle='--')
plt.axvline(0, color='black', linestyle=':')
plt.title('Zoomed Distribution (-0.2 ~ 0.2)')
plt.xlabel('Score')
plt.ylabel('Document Count')
plt.show()

def label_doc(score, th=0.05):
    if score >= th:
        return 'hawkish'
    if score <= -th:
        return 'dovish'
    return 'neutral'

df_keep['doc_label'] = df_keep[score_col].apply(label_doc)

label_counts = df_keep['doc_label'].value_counts(normalize=True)

plt.figure(figsize=(6, 4))
label_counts.plot(kind='bar')
plt.title('Document Label Distribution')
plt.ylabel('Proportion')
plt.xticks(rotation=0)
plt.show()

label_counts

df_keep.sort_values(score_col, ascending=False).head(10)[
    [score_col, 'topic_id', 'text_for_topic']
]

df_keep.sort_values(score_col).head(10)[
    [score_col, 'topic_id', 'text_for_topic']
]

import matplotlib.pyplot as plt

label_counts_cnt = df_keep['doc_label'].value_counts()  # normalize=False (ê¸°ë³¸ê°’)

plt.figure(figsize=(6, 4))
label_counts_cnt.plot(kind='bar')
plt.title('Document Label Distribution (Count)')
plt.ylabel('Document Count')
plt.xlabel('doc_label')
plt.xticks(rotation=0)
plt.show()

label_counts_cnt

pd.DataFrame({
    'count': label_counts_cnt,
    'proportion': label_counts_cnt / label_counts_cnt.sum()
})

def label_doc_asymmetric(score):
    if score >= 0.05:
        return 'hawkish'
    elif score <= -0.02:
        return 'dovish'
    else:
        return 'neutral'

df_keep['doc_label'] = df_keep['doc_hawk_dove_score_prob'].apply(label_doc_asymmetric)

df_keep['doc_label'].value_counts(), df_keep['doc_label'].value_counts(normalize=True)

df_keep.shape[1]

df_keep.columns.tolist()

keep_cols = [
    'doc_id',                     # ë¬¸ì„œ ì‹ë³„ì
    'cleaned_text_lemma',          # ì „ì²˜ë¦¬ëœ ì›ë¬¸
    'text_for_topic',              # collocation ë°˜ì˜ í…ìŠ¤íŠ¸
    'topic_id',                    # BERTopic ê²°ê³¼
    'doc_hawk_dove_score_prob',    # í™•ë¥  ê¸°ë°˜ ë§¤/ë¹„ ì ìˆ˜ (-1~1)
    'doc_label'                    # hawkish / dovish / neutral
]

df_final = df_keep[keep_cols].copy()

print('final columns:', df_final.columns.tolist())
print('shape:', df_final.shape)
df_final.head()

df_final.to_csv(
    'df_final_hawk_dove_topic_prob.csv',
    index=False
)

print('saved : df_final_hawk_dove_topic_prob.csv')

import pandas as pd
import matplotlib.pyplot as plt

# df_keepì— doc_label ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
# (ì—†ìœ¼ë©´: df_keep['doc_label'] = ... ë¼ë²¨ë§ ë¨¼ì €)

# 1) count ê³„ì‚°
counts = df_keep['doc_label'].value_counts()

# 2) ë§‰ëŒ€ê·¸ë˜í”„ (y=ê°œìˆ˜)
plt.figure(figsize=(9, 5))
plt.bar(counts.index, counts.values)
plt.title('Document Label Distribution (Count)')
plt.xlabel('doc_label')
plt.ylabel('Document Count')
plt.show()

# 3) í‘œë¡œ ì¶œë ¥ (ìŠ¤í¬ë¦°ìƒ·ì²˜ëŸ¼ "count" í—¤ë” í˜•íƒœ)
print('count')
display(counts.to_frame())

