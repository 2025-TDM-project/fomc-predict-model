{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609afa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://www.federalreserve.gov\"\n",
    "\n",
    "def fetch_minutes_urls_historical_grouped(start_year=1993, end_year=2019):\n",
    "    grouped_urls = {}\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"{BASE}/monetarypolicy/fomchistorical{year}.htm\"\n",
    "        #print(f\"ğŸ“„ í¬ë¡¤ë§ ì¤‘: {url}\")\n",
    "\n",
    "        res = requests.get(url)\n",
    "        if res.status_code != 200:\n",
    "            print(f\"âš ï¸ {year} í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        year_urls = []\n",
    "\n",
    "        # Minutes / Minutes of Actions ì°¾ê¸°\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].lower()\n",
    "\n",
    "            if not href.endswith(\".htm\"):\n",
    "                continue\n",
    "\n",
    "            if (\"minutes\" in href) or (\"actions\" in href):\n",
    "                full = urljoin(BASE, a[\"href\"])\n",
    "                year_urls.append(full)\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° + ì •ë ¬\n",
    "        grouped_urls[str(year)] = sorted(list(set(year_urls)))\n",
    "\n",
    "    return grouped_urls\n",
    "\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "grouped_minutes = fetch_minutes_urls_historical_grouped(1993, 2019)\n",
    "\n",
    "# âœ… ì—°ë„ë³„ ê°œìˆ˜ ì¶œë ¥\n",
    "for year, urls in grouped_minutes.items():\n",
    "    print(year, \":\", len(urls))\n",
    "\n",
    "# âœ… ì˜ˆì‹œ: íŠ¹ì • ì—°ë„ URL ë³´ê¸°\n",
    "print(\"\\n1993ë…„ URL ã…¡>\")\n",
    "for u in grouped_minutes[\"1994\"]:\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì˜ˆì‹œ: íŠ¹ì • ì—°ë„ URL ë³´ê¸°\n",
    "# ì¶œë ¥í•˜ê³  ì‹¶ì€ ì—°ë„ ë²”ìœ„\n",
    "start_year = 2011\n",
    "end_year = 2019\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    year_str = str(year)\n",
    "    if year_str in grouped_minutes:\n",
    "        print(f\"\\nğŸ“Œ {year}ë…„ Minutes\")\n",
    "        for url in grouped_minutes[year_str]:\n",
    "            print(url)\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ {year}ë…„ ë°ì´í„° ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = []\n",
    "\n",
    "for url in urls:\n",
    "    print(\"\\nì²˜ë¦¬ ì¤‘:\", url)\n",
    "\n",
    "    # ë‚ ì§œ ì¶”ì¶œ\n",
    "    date_match = re.search(r\"(\\d{8})\", url)\n",
    "    date = date_match.group(1) if date_match else \"unknown\"\n",
    "    doc_id = f\"FOMC_{date}\"\n",
    "\n",
    "    # âœ… ì¤‘ë³µ ë°©ì§€\n",
    "    if doc_id in existing_doc_ids:\n",
    "        print(f\"â›” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ â€” ê±´ë„ˆëœ€: {doc_id}\")\n",
    "        continue\n",
    "\n",
    "    raw_text, soup = fetch_minutes_text(url)\n",
    "\n",
    "    title_tag = soup.find(\"h3\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"FOMC Minutes\"\n",
    "\n",
    "    doc_type = \"minutes\"\n",
    "    source = \"Federal Reserve\"\n",
    "    language = \"en\"\n",
    "\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "\n",
    "    doc_tokens = []\n",
    "    doc_pos_tags = []\n",
    "    doc_cleaned_sentences = []\n",
    "    doc_lemmas = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens, pos_tags, cleaned, lemmas = preprocess_sentence(sent)\n",
    "        doc_tokens.append(tokens)\n",
    "        doc_pos_tags.append(pos_tags)\n",
    "        doc_cleaned_sentences.append(cleaned)\n",
    "        doc_lemmas.extend(lemmas)\n",
    "\n",
    "    row = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"doc_type\": doc_type,\n",
    "        \"date\": date,\n",
    "        \"title\": title,\n",
    "        \"source\": source,\n",
    "        \"language\": language,\n",
    "        # âœ… ì¤„ë°”ê¿ˆ ì œê±° â†’ CSV í•œ ì…€ì— ê¹”ë” ì €ì¥\n",
    "        \"raw_text\": raw_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \"),\n",
    "        \"sentences\": json.dumps(sentences, ensure_ascii=False),\n",
    "        \"tokens\": json.dumps(doc_tokens, ensure_ascii=False),\n",
    "        \"pos_tags\": json.dumps(doc_pos_tags, ensure_ascii=False),\n",
    "        \"cleaned_sentences\": json.dumps(doc_cleaned_sentences, ensure_ascii=False),\n",
    "        \"lemmas\": json.dumps(doc_lemmas, ensure_ascii=False)\n",
    "    }\n",
    "\n",
    "    new_rows.append(row)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
