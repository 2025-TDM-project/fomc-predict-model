import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import numpy as np
import re
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
nltk.download('averaged_perceptron_tagger_eng', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)
nltk.download('punkt', quiet=True)

#import df
df=pd.read_csv(r'C:\TMD_Project\df_master_TM_proj.csv')

#Lemmatizing

# 2. Lemmatizer ì´ˆê¸°í™”
lemmatizer = WordNetLemmatizer()

# 3. POS tagë¥¼ WordNet tagë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN


# 4. Lemmatization í•¨ìˆ˜
def lemmatize_text(text):
    if pd.isna(text):
        return ''

    words = text.split()
    pos_tags = nltk.pos_tag(words)
    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos))
                  for word, pos in pos_tags]
    return ' '.join(lemmatized)

# 5. ë°ì´í„°í”„ë ˆì„ì— ì ìš©
df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)


#Collocation
#ì—°ì–´ ëª¨ìŒ ë¶ˆëŸ¬ì˜¤ê¸°
import pickle

file_path=r'C:\TMD_Project\collocation_list.pkl'
# 'rb': ì½ê¸° ëª¨ë“œ (Read Binary)
with open(file_path, 'rb') as f:
    collocation_text_list = pickle.load(f)
collocation_set = set(collocation_text_list)

def collocation_words(col):
    words = str(col).split(', ')
    return ' '.join([word for word in words if word in collocation_set])

df['distilled_by_collocation']=df['lemmatized_text'].apply(collocation_words)


#Sentimental Analysis
# --- 1. ë°ì´í„° ì „ì²˜ë¦¬ (ë‹¨ì–´ ì¶”ì¶œ ë° í† í°í™”) ---

# df['cleaned_text'] ì—´ì´ 'time inflation year'ì™€ ê°™ì€ ë¬¸ìì—´ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ì´ë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
def tokenize_string(text):
    return str(text).split()

# ìƒˆë¡œìš´ ì—´ì— í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ ì €ì¥ (ì´ ì—´ì„ í•™ìŠµì— ì‚¬ìš©í•©ë‹ˆë‹¤)
df['tokenized_collocation'] = df['cleaned_text'].apply(tokenize_string)

# --- 2. Word2Vec ëª¨ë¸ í•™ìŠµ ---

# Word2Vec ì…ë ¥: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ [[w1, w2], [w3, w4], ...]
training_data = df['tokenized_collocation'].tolist()

print(f"í•™ìŠµ ë°ì´í„° ì˜ˆì‹œ (ì²« 2ê°œ í–‰): {training_data[:2]}")
#

# ëª¨ë¸ í•™ìŠµ
model = Word2Vec(sentences=training_data,
                 vector_size=100,
                 window=10,      # í•œ í–‰ì˜ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ê´€ë ¨ìˆë‹¤ê³  ê°€ì •
                 min_count=1,
                 workers=4,
                 sg=1)           # sg=1: Skip-gram (ì˜ë¯¸ë¡ ì  ê´€ê³„ íŒŒì•…ì— ìœ ë¦¬)

# --- 3. ì”¨ì•— ë‹¨ì–´ ì •ì˜ ë° í•„í„°ë§ ---
# ë§¤íŒŒ (Hawkish) - ê¸´ì¶• ì •ì±… ì„ í˜¸
seeds_hawk= [
    'inflation', 'tightening', 'rate', 'hike', 'increase', 'restrict',
    'hawkish', 'aggressive', 'curb', 'combat', 'control', 'restrain',
    'normalize', 'normalization', 'contractionary', 'raising', 'higher',
    'cool', 'cooling', 'brake', 'slowdown', 'decelerate',
    'overheat', 'overheating', 'pressure', 'concern', 'worry',
    'vigilant', 'cautious', 'prudent', 'tough', 'firm',
    'withdraw', 'withdrawal', 'taper', 'tapering', 'reduce',
    'unwind', 'unwinding', 'drain', 'shrink', 'balance', 'sheet', 'reduction',
    'quantitative', 'tightening', 'terminal', 'rate', 'neutral', 'rate',
    'upward', 'lift', 'elevated', 'persistent', 'sticky',

    # ğŸŒŸ ì¶”ê°€ëœ ë‹¨ì–´
    'front',' load', 'expeditious', 'unwavering', 'decisive', 'pre','emptive', 'successive', 'above', 'neutral',
    'underlying', 'inflation', 'price','stability', 'inflationary', 'pressures', 'wage', 'growth', 'second','round', 'effects', 'tight', 'labor', 'market',
    'vigilance', 'resolve', 'intransigent', 'intractable', 'commitment', 'credibility',
    'selling','assets', 'runoff', 'draining', 'reserves', 'real rate',
    'necessary', 'evil', 'painful', 'adjustment', 'imperative', 'non','negotiable'
]

# ë¹„ë‘˜ê¸°íŒŒ (Dovish) - ì™„í™” ì •ì±… ì„ í˜¸
seeds_dove= [
    'growth', 'cut', 'easing', 'stimulus', 'reduction', 'lower',
    'dovish', 'accommodative', 'support', 'boost', 'encourage', 'promote',
    'expansionary', 'lowering', 'decrease', 'ease', 'relax',
    'patient', 'gradual', 'cautious', 'measured', 'pause',
    'hold', 'steady', 'maintain', 'wait', 'monitor',
    'recovery', 'employment', 'jobs', 'unemployment', 'labor',
    'weak', 'weakness', 'soft', 'softness', 'slow',
    'dovish', 'inject', 'injection', 'liquidity', 'provide',
    'quantitative easing', 'asset purchase', 'accommodation',
    'downward', 'decline', 'subdued', 'moderate', 'muted',
    'flexible', 'data dependent', 'stabilize', 'stability',

    # ğŸŒŸ ì¶”ê°€ëœ ë‹¨ì–´
    'supportive', 'extraordinary', 'measures', 'forward', 'guidance', 'open','ended', 'reinvest',
    'maximum', 'employment', 'slack', 'underutilized', 'resources', 'headwinds', 'transitory', 'symmetric','target',
    'flexibility', 'wait','and','see', 'patiently', 'adjust', 'contingent',
    'downside', 'risks', 'uncertain', 'outlook', 'fragile', 'recovery', 'output', 'gap',
    'benign', 'contained', 'manageable', 'well','anchored'
]

# ëª¨ë¸ ë‹¨ì–´ ì‚¬ì „ì— ìˆëŠ” ì”¨ì•— ë‹¨ì–´ë§Œ ìœ íš¨í•˜ê²Œ ë‚¨ê¹€
valid_hawk = [w for w in seeds_hawk if w in model.wv]
valid_dove = [w for w in seeds_dove if w in model.wv]

print(f"ìœ íš¨í•œ ë§¤íŒŒ ì”¨ì•—: {valid_hawk}")
print(f"ìœ íš¨í•œ ë¹„ë‘˜ê¸°íŒŒ ì”¨ì•—: {valid_dove}")

# --- 4. ê·¹ì„± ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ---

def get_polarity_score(target_word, model, hawks, doves):
    """
    ë‹¨ì–´ì˜ ê·¹ì„± ì ìˆ˜ë¥¼ ê³„ì‚° (ë§¤íŒŒ ìœ ì‚¬ë„ - ë¹„ë‘˜ê¸°íŒŒ ìœ ì‚¬ë„)
    """
    if target_word not in model.wv or not hawks or not doves:
        return 0.0

    # ë§¤íŒŒ ê·¸ë£¹ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„
    sim_hawk = np.mean([model.wv.similarity(target_word, h) for h in hawks])
    # ë¹„ë‘˜ê¸°íŒŒ ê·¸ë£¹ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„
    sim_dove = np.mean([model.wv.similarity(target_word, d) for d in doves])

    # ê·¹ì„± ì ìˆ˜ (ì–‘ìˆ˜: ë§¤íŒŒì , ìŒìˆ˜: ë¹„ë‘˜ê¸°íŒŒì )
    return sim_hawk - sim_dove

def analyze_row_polarity(word_list):
    """
    í•œ í–‰(ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸)ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ê·¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    """
    row_results = {}
    for word in word_list:
        score = get_polarity_score(word, model, valid_hawk, valid_dove)
        # 0ì´ ì•„ë‹Œ ìœ ì˜ë¯¸í•œ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
        if score != 0:
            row_results[word] = round(score, 4)
    return row_results

# --- 5. ê²°ê³¼ ì ìš© (ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±) ---

# í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ìˆ˜í–‰
df['polarity_analysis_result'] = df['tokenized_collocation'].apply(analyze_row_polarity)

# --- ê²°ê³¼ í™•ì¸ ---
pd.set_option('display.max_colwidth', None) # ë‚´ìš© ì˜ë¦¼ ë°©ì§€
print("\n=== ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 5í–‰) ===")
print(df[['cleaned_text', 'polarity_analysis_result']].head())

# (ì„ íƒ) ì „ì²´ í–‰ì— ëŒ€í•œ í‰ê·  ë§¤íŒŒ/ë¹„ë‘˜ê¸°íŒŒ ì„±í–¥ ì ìˆ˜ ê³„ì‚° ì˜ˆì‹œ
def calculate_aggregate_score(result_dict):
    """
    ë¬¸ì„œì— í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´ì˜ ê·¹ì„± ì ìˆ˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë¬¸ì„œ ì „ì²´ ì„±í–¥ ì ìˆ˜ë¥¼ ì‚°ì¶œ
    """
    if not result_dict:
        return 0.0
    return np.mean(list(result_dict.values()))

df['doc_sentiment_score'] = df['polarity_analysis_result'].apply(calculate_aggregate_score)

print("\n=== ë¬¸ì„œ ì „ì²´ ì„±í–¥ ì ìˆ˜ (ìƒìœ„ 5í–‰) ===")
print(df[['cleaned_text', 'doc_sentiment_score']].head())

df.to_csv(r'C:\TMD_Project\df_TM_Lem_Col_TF.csv', index=False)
